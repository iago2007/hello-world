資訊理論與編碼技巧 HW1 r03944034 楊至柔
==
### 2-1
* (a)
    $\begin{aligned}
    X&:\{1,2,3,...,N,...\}\\p(X)&:\{\frac{1}{2},\frac{1}{4},\frac{1}{8},...,\frac{1}{2^n},...\}\\H(X)&=E_p\log(\frac{1}{p(X)})\\&=\sum_{n=1}^\infty\frac{1}{2^n}\cdot n\\&=\sum_{m=1}^\infty\sum_{n=m}^\infty\frac{1}{2^n}\\&=\sum_{m=1}^\infty\frac{1}{2^{m+1}}\\&=2
    \end{aligned}$
* (b)
Use the sequence of yes-no questions: "Is $X$ equals $1$?", "Is $X$ equals $2$?", "Is $X$ equals $3$?"... there is $\frac{1}{2}$ chance that it guess right in the first question, $\frac{1}{4}$ chance that it guess right in the second question, and so on. So the expected number of questions required to determine X is $\frac{1}{2}\cdot1+\frac{1}{4}\cdot2+\frac{1}{8}\cdot3+...=2$, equals to H(X).
---
### 2-3 

$\begin{aligned}
H(p)&=E_p\log(\frac{1}{p(X)})\\&=\sum_{i=1}^n-p_i\log p_i\\&\geq0
\end{aligned}$
and $H(p)=0$ only if for every $p_i$, $p_i=0$ or $p_i=1$ and there is n vectors that all elements is 0 or 1. that is $(1,0,0,...,0),(0,1,0,...,0),(0,0,1,...,0),...,(0,0,0...,1)$

---
### 2-4
* (a)
by chain rule of entropy (Theorem 2.2.1)
* (b)
since for every possible value of X, there is only one possible value of g(X), $H(g(X)|X)=0$
* (c\)
by chain rule of entropy (Theorem 2.2.1)
* (d)
by Lemma 2.2.1, $H(X|g(X))\geq0$
---
### 2-5
$H(Y|X)=\sum_{x\in \mathcal X}p(x)H(Y|X=x)$
since $p(x)>0$ , $H(Y|X)=0$ implys that $\forall x\in \mathcal X, H(Y|X=x)=0$
$\Rightarrow$ for every possible x, the distribution of Y given x can not have 2 different value or $H(Y|X=x)$ will not equal to 0. $\Rightarrow Y$ is a function of $X$

---
### 2-10

* (a)
    $\begin{aligned}
H(X)&=E_p\log(\frac{1}{p(x)})\\&=\sum_{x\in X_1}\alpha p_1(x)\frac{1}{\log(\alpha p_1(x))}+\sum_{x\in X_2}(1-\alpha) p_2(x)\frac{1}{\log((1- \alpha) p_2(x))}\\&=-\alpha \sum_{x\in X_1}p_1(x)(\log(\alpha)+\log(p_1(x)))-(1-\alpha) \sum_{x\in X_2}p_2(x)(\log(1-\alpha)+\log(p_2(x)))\\&=-\alpha\log(\alpha)+\alpha H(X_1)-(1-\alpha)\log(1-\alpha)+(1-\alpha)H(X_2)\\&=H(\alpha)+\alpha H(X_1)+(1-\alpha)H(X_2)
\end{aligned}$

* (b)


    
---
### 2-12 

* (a)
    $\begin{aligned}
    H(X)=E_p\log\frac{1}{p(X)}\approx0.91830\\H(Y)=E_p\log\frac{1}{p(Y)}\approx0.91830
    \end{aligned}$
* (b)
    $\begin{aligned}
    H(X|Y)&=E_p\log\frac{1}{p(X|Y)}\\&=\frac{1}{3}\cdot\log1+\frac{1}{3}\cdot\log 2+\frac{1}{3}\cdot\log2\\&\approx0.66667
    \end{aligned}$
    $\begin{aligned}
    H(Y|X)&=E_p\log\frac{1}{p(Y|X)}\\&=\frac{1}{3}\cdot\log1+\frac{1}{3}\cdot\log 2+\frac{1}{3}\cdot\log2\\&\approx0.66667
    \end{aligned}$
* (c\)
    $\begin{aligned}
    H(X,Y)&=E_p\log\frac{1}{p(X,Y)}\\&=\frac{1}{3}\cdot\log3+\frac{1}{3}\cdot\log3+\frac{1}{3}\cdot\log3\\&\approx1.58496
    \end{aligned}$
* (d)
    $\begin{aligned}
    H(Y)-H(Y|X)&\approx0.25163
    \end{aligned}$
* (e)
    $\begin{aligned}
    I(X;Y)&=E_p\log\frac{p(X,Y)}{p(X)p(
    Y)}\\&=\frac{1}{3}\cdot\log\frac{\frac{1}{3}}{\frac{2}{3}\cdot\frac{1}{3}}+\frac{1}{3}\cdot\log\frac{\frac{1}{3}}{\frac{2}{3}\cdot\frac{1}{3}}+\frac{1}{3}\cdot\log\frac{\frac{1}{3}}{\frac{2}{3}\cdot\frac{2}{3}}\\&\approx0.25163
    \end{aligned}$
* (f) 

![test](https://i.imgur.com/Lhsg6Fg.png)



---
### 2-16

* (a)
since $X_1 \rightarrow X_2 \rightarrow X_3$,
$\begin{aligned}
I(X_1;X_3)&\leq I(X_1;X_2)\\&=H(X_2)-H(X_2|X_1)\\&\leq H(X_2)\leq \log(k)
\end{aligned}$
the dependence of $X_1$ and $X_3$ is limited by the bottleneck.
* (b)
    for k=1, we have $I(X_1,X_3)\leq\log1=0$ , and since mutual information always larger or equal to zero, we have $I(X_1,X_3)=0$ ,$X_1$ and $X_3$ ar independent.

---
### 2-18
* Start with the number of game played by both team.
 A win 4 and B win 0
 0 multisubsets of 4 elements ,${3\choose 0}=1$ possible way with probability $\frac{1}{16}$
    A win 4 and B win 1
        1 multisubsets of 4 elements ,${4\choose 1}=4$ possible way with probability $\frac{1}{32}$
    A win 4 and B win 2
        2 multisubsets of 4 elements ,${5\choose 2}=10$ possible way with probability $\frac{1}{64}$
     A win 4 and B win 3
        3 multisubsets of 4 elements ,${6\choose 3}=20$ possible way with probability $\frac{1}{128}$
and same calculation as B wins.

$\begin{aligned}
H(X)&=E_p\log\frac{1}{p(X)}\\&=2\cdot\frac{1}{16}\cdot4+8\cdot\frac{1}{32}\cdot5+20\cdot\frac{1}{64}\cdot6+40\cdot\frac{1}{128}\cdot7\\&=5.8125
\end{aligned}$
$\begin{aligned}
H(Y)&=E_p\log\frac{1}{p(Y)}\\&=\frac{2}{16}\cdot\log(\frac{16}{2})+\frac{8}{32}\cdot\log(\frac{32}{8})+\frac{20}{64}\cdot\log(\frac{64}{20})+\frac{40}{128}\cdot\log(\frac{128}{40})\\&\approx1.92379
\end{aligned}$
since for every possible $X$ there is only 1 possible $Y$ value,
$H(Y|X)=0$
$H(X|Y)=H(X,Y)-H(Y)=H(X)+H(Y|X)-H(Y)\approx3.88871$


---
### 2-25

* (a)
$\begin{aligned}
    I(X;Y;Z)&=I(X;Y)-I(X;Y|Z)\\&=I(X;Y)-(I(X;Y,Z)-I(X;Z))\\&=I(X;Y)+I(X;Z)-I(X;Y,Z)\\&=I(X;Y)+I(X;Z)-(H(X)+H(Y,Z)-H(X,Y,Z))\\&=I(X;Y)+I(X;Z)-H(X)-H(Y,Z)+H(X,Y,Z)\\&=I(X;Y)+I(X;Z)-H(X)-(H(Y)+H(Z)-I(Y;Z))+H(X,Y,Z)\\&=H(X,Y,Z)-H(X)-H(Y)-H(Z)+I(X;Y)+I(X;Z)+I(Y;Z)
\end{aligned}$

* (b)
$\begin{aligned}
    I(X;Y;Z)&=H(X,Y,Z)-H(X)-H(Y)-H(Z)+I(X;Y)+I(X;Z)+I(Y;Z)\\&=H(X,Y,Z)-H(X)-H(Y)-H(Z)\\&+(H(X)+H(Y)-H(X,Y))+(H(X)+H(Z)-H(X,Z))\\&+(H(Y)+H(Z)-H(Y,Z))\\&=H(X,Y,Z)-H(X,Y)-H(X,Z)-H(Y,Z)+H(X)+H(Y)+H(Z)
\end{aligned}$


---
### 2-29

* (a)
    $H(X,Y|Z)=H(X|Z)+H(Y|X,Z)\geq H(X,Z)$, equality if $H(Y|X,Z)=0$
* (b)
    $I(X,Y;Z)=I(X;Z)+I(Y;Z|X)\geq I(X;Z)$, equality if $I(Y;Z|X)=0$
* (c\)
    $\begin{aligned}
    H(X,Y,Z)-H(X,Y)&=H(Z|X,Y)\\&= H(Z|X)-I(Z;Y|X)\\&\leq H(Z|X)=H(X,Z)-H(X)
    \end{aligned}$
    with equality if $I(Y;Z|X)=0$
* (d)
    $I(X;Z|Y)=I(X,Y;Z)-I(Y;Z)=(I(Y;Z|X)+I(X;Z))-I(Y;Z)$
    and equal in all cases.

---
### 2-32

* (a)
    If we estimate $X$ based on $Y$ , the best guess is 
    $1$ when $Y=a$
    $2$ when $Y=b$
    $3$ when $Y=c$
    and the minimum probability of error $P_e= 1-(1/6+1/6+1/6)=\frac{1}{2}$
    
* (b)
    From Fano's inequality, We have
    $\begin{aligned}
    P_e&\geq \frac{H(X|Y)-1}{\log \mathcal X}\\&=\frac{\sum_k H(X|Y=k)\cdot P(Y=k)-1}{\log3}\\&=(1.5-1)/\log3=0.3155
     \end{aligned}$
 

---
### 2-35
$\begin{aligned}
H(p)&=E_p\log(\frac{1}{p(X)})=\frac{1}{2}\cdot1+\frac{1}{4}\cdot2+\frac{1}{4}\cdot2=1.5\\
H(q)&=E_q\log(\frac{1}{q(X)})=\frac{1}{3}\cdot\log3+\frac{1}{3}\cdot\log3+\frac{1}{3}\cdot\log3\approx1.58496\\
D(p||q)&=E_p\log(\frac{p(X)}{q(X)})=\frac{1}{2}\cdot\log\frac{3}{2}+\frac{1}{4}\cdot\log\frac{3}{4}+\frac{1}{4}\cdot\log\frac{3}{4}\approx0.08496\\
D(q||p)&=E_q\log(\frac{q(X)}{p(X)})=\frac{1}{3}\cdot\log\frac{2}{3}+\frac{1}{3}\cdot\log\frac{4}{3}+\frac{1}{3}\cdot\log\frac{4}{3}\approx0.08170\\
D(p||q)&\neq D(q||p)
\end{aligned}$

---

